{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for Fashion-MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project utilizes keras framework of python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dense, Dropout, Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.random.seed(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data from CSV files "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_file = 'fashion-mnist_train.csv' #data for train set\n",
    "test_file = 'fashion-mnist_test.csv'# data for test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train = read_csv(train_file)\n",
    "df_test = read_csv(test_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ar = df_train.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = ar[:, 1:] \n",
    "Y_train = ar[:, 0] #since first row in the dataset belongs to the labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000,)\n"
     ]
    }
   ],
   "source": [
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pixel_w = 28\n",
    "pixel_h = 28 #each image is of 28x28 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], pixel_w, pixel_h, 1) # reshaping the flattened image values in square of 28x28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "array = df_test.values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = array[:, 1:]\n",
    "Y_test = array[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n"
     ]
    }
   ],
   "source": [
    "print(Y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(X_test.shape[0], pixel_w, pixel_h, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_shape = (pixel_w, pixel_h, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32') #converting the RGB values in float type, the one accepted by CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = X_test.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train /= 255\n",
    "X_test /= 255   #to convert into percentage form "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train = keras.utils.to_categorical(Y_train, 10) # turning the labels into categorical binary form eg, Y having label as 2 caan represented  as 0010000000   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_test = keras.utils.to_categorical(Y_test, 10) #number of classes are 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building CNN using the Sequential model of Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post Conolution layer: (None, 26, 26, 32)\n",
      "Post MaxPooling layer: (None, 13, 13, 32)\n",
      "Post Droput Layer: (None, 13, 13, 32)\n",
      "Post flatten: (None, 5408)\n",
      "Post Dense: (None, 128)\n",
      "Post softmax: (None, 10)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3,3), activation = 'relu', input_shape = input_shape)) #convolution layer- Using relu to make sure that  there are no negative values\n",
    "print(\"Post Conolution layer:\", model.output_shape)\n",
    "model.add(MaxPooling2D(pool_size = (2,2))) # Pooling layer\n",
    "print(\"Post MaxPooling layer:\", model.output_shape)\n",
    "model.add(Dropout(0.30)) #dropout Layer to avoid overfitting\n",
    "print(\"Post Droput Layer:\", model.output_shape)\n",
    "model.add(Flatten()) #fully connected layer 1\n",
    "print(\"Post flatten:\", model.output_shape)\n",
    "model.add(Dense(128, activation = 'relu', )) # fully connected layer 2\n",
    "print('Post Dense:', model.output_shape)\n",
    "model.add(Dense(10, activation = 'softmax')) # since data set has 10 labels\n",
    "print('Post softmax:' , model.output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compiling the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss = keras.losses.categorical_crossentropy, optimizer= keras.optimizers.Adadelta(), metrics = ['accuracy']) #since Adadelta is best recommended optimizer for images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 139s 2ms/step - loss: 0.4231 - acc: 0.8485 - val_loss: 0.3055 - val_acc: 0.8921\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 138s 2ms/step - loss: 0.2989 - acc: 0.8926 - val_loss: 0.2720 - val_acc: 0.9027\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 138s 2ms/step - loss: 0.2658 - acc: 0.9026 - val_loss: 0.2514 - val_acc: 0.9117\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 139s 2ms/step - loss: 0.2425 - acc: 0.9126 - val_loss: 0.2389 - val_acc: 0.9134\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 139s 2ms/step - loss: 0.2237 - acc: 0.9188 - val_loss: 0.2325 - val_acc: 0.9173\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 138s 2ms/step - loss: 0.2083 - acc: 0.9245 - val_loss: 0.2237 - val_acc: 0.9213\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 140s 2ms/step - loss: 0.1980 - acc: 0.9273 - val_loss: 0.2239 - val_acc: 0.9207\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 140s 2ms/step - loss: 0.1871 - acc: 0.9303 - val_loss: 0.2179 - val_acc: 0.9219\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 141s 2ms/step - loss: 0.1776 - acc: 0.9354 - val_loss: 0.2219 - val_acc: 0.9229\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 140s 2ms/step - loss: 0.1703 - acc: 0.9379 - val_loss: 0.2336 - val_acc: 0.9202\n",
      "10000/10000 [==============================] - 4s 444us/step\n",
      "0.23360323844\n"
     ]
    }
   ],
   "source": [
    "model.fit(X_train, Y_train, batch_size = batch_size, epochs = epochs, verbose =1, validation_data = (X_test, Y_test) )\n",
    "score = model.evaluate(X_test, Y_test, verbose = 1)\n",
    "print(score[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc :92.02%\n"
     ]
    }
   ],
   "source": [
    "print(\"%s :%.2f%%\" % (model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :0.23360\n"
     ]
    }
   ],
   "source": [
    "print(\"%s :%.5f\" %(model.metrics_names[0], score[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing trained model from an input image of test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8BM59X8DWmbEo0hSsQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x264a3177320>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_im = X_train[None, 1]\n",
    "plt.imshow(test_im.reshape(28,28), cmap='viridis', interpolation='none')\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p = model.predict_classes(test_im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9]\n"
     ]
    }
   ],
   "source": [
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is of a ankle boot\n"
     ]
    }
   ],
   "source": [
    "if p == 0:\n",
    "    print(\"The image is of a T-shirt/Top\")\n",
    "if p == 1:\n",
    "    print(\"The image is of a Trouser\")\n",
    "if p == 2:\n",
    "    print(\"The image is of a pullover\")\n",
    "if p == 3:\n",
    "    print(\"The image is of a dress\")\n",
    "if p == 4:\n",
    "    print(\"The image is of a coat\")\n",
    "if p == 5:\n",
    "    print(\"The image is of a sandal\")\n",
    "if p == 6:\n",
    "    print(\"The image is of a shirt\")\n",
    "if p == 7:\n",
    "    print(\"The image is of a sneaker\")\n",
    "if p == 8:\n",
    "    print(\"The image is of a bag\")\n",
    "if p == 9:\n",
    "    print(\"The image is of a ankle boot\") #assigned values as per instruction on kaggles webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
